@InProceedings{wagner2021learning,
  author="Wagner, Stefan
and Janschek, Michael
and Uelwer, Tobias
and Harmeling, Stefan",
  editor="Farka{\v{s}}, Igor
and Masulli, Paolo
and Otte, Sebastian
and Wermter, Stefan",
  title="Learning to Plan via a Multi-step Policy Regression Method",
booktitle="Artificial Neural Networks and Machine Learning -- ICANN 2021",
  year="2021",
  publisher="Springer International Publishing",
  address="Cham",
  pages="481--492",
  abstract="We propose a new approach to increase inference performance in environments that require a specific sequence of actions in order to be solved. This is for example the case for maze environments where ideally an optimal path is determined. Instead of learning a policy for a single step, we want to learn a policy that can predict n actions in advance. Our proposed method called policy horizon regression (PHR) uses knowledge of the environment sampled byA2C to learn an n dimensional policy vector in a policy distillation setup which yields n sequential actions per observation. We test our method on the MiniGrid and Pong environments and show drastic speedup during inference time by successfully predicting sequences of actions on a single observation.",
  isbn="978-3-030-86380-7"
}

